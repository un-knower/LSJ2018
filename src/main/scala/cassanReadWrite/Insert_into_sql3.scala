package cassanReadWrite

import libsvm.PackageChecker
import org.apache.spark.sql.functions.expr
import org.apache.spark.sql.{SaveMode, SparkSession}
//import libsvm.PackageChecker
import org.apache.spark.SparkConf

object Insert_into_sql3 {
  def main(args: Array[String]): Unit = {

    val packages = List("org.apache.spark:spark-core_2.21:2.2.0",
      "org.apache.spark:spark-sql_2.11:2.2.0",
      "com.datastax.spark:spark-cassandra-connector_2.11:2.0.3")
    val jars = new PackageChecker(packages.mkString(","), "", "").getJar()
    val alljars = jars.split(",")

    val conf = new SparkConf()
      .setMaster("spark://10.90.9.111:7077")
      .setAppName("Uimge_manual_app_insert")
      .set("spark.cassandra.connection.host", "10.80.17.155")
      .set("spark.executor.memory", "30g")
      .set("spark.cores.max", "280")
      .setJars(alljars)

    val spark = SparkSession.builder().config(conf).getOrCreate()
    import spark.implicits._
    //    val df6 = spark.read.format("org.apache.spark.sql.cassandra")
    //      .options(Map("table" -> "new_education_group", "keyspace" -> "groups"))
    //      .load().sort($"label").limit(8000000)
    //      .select($"uid", expr("0").alias("d601")
    //        , expr("0").alias("d602")
    //        , expr("0").alias("d603")
    //        , expr("0").alias("d604")
    //        , expr("0").alias("d605")
    //        , expr("0").alias("d606")
    //        , expr("0").alias("d607")
    //        , expr("0").alias("d608")
    //        , expr("0").alias("d609")
    //        , expr("0").alias("d610"))
    //    df6.write.format("org.apache.spark.sql.cassandra")
    //      .options(Map("table" -> "uimge_manual_app2", "keyspace" -> "uimg"))
    //      .mode(SaveMode.Append).save()

//    val df7 = spark.read.format("org.apache.spark.sql.cassandra")
//      .options(Map("table" -> "new_travel_group", "keyspace" -> "groups"))
//      .load().sort($"label").limit(10000000)
//      .select($"uid", expr("0").alias("d701")
//        , expr("0").alias("d702")
//        , expr("0").alias("d703")
//        , expr("0").alias("d704")
//        , expr("0").alias("d705")
//        , expr("0").alias("d706")
//        , expr("0").alias("d707")
//        , expr("0").alias("d708"))
//    df7.write.format("org.apache.spark.sql.cassandra")
//      .options(Map("table" -> "uimge_manual_app2", "keyspace" -> "uimg"))
//      .mode(SaveMode.Append).save()
//    val df8 = spark.read.format("org.apache.spark.sql.cassandra")
//      .options(Map("table" -> "new_finance_group", "keyspace" -> "groups"))
//      .load().sort($"label").limit(3000000)
//      .select($"uid", expr("0").alias("d801")
//        , expr("0").alias("d802")
//        , expr("0").alias("d803")
//        , expr("0").alias("d804")
//        , expr("0").alias("d805")
//        , expr("0").alias("d806")
//        , expr("0").alias("d807"))
//    df8.write.format("org.apache.spark.sql.cassandra")
//      .options(Map("table" -> "uimge_manual_app2", "keyspace" -> "uimg"))
//      .mode(SaveMode.Append).save()
//    val df9 = spark.read.format("org.apache.spark.sql.cassandra")
//      .options(Map("table" -> "new_vehicle_group", "keyspace" -> "groups"))
//      .load().sort($"label").limit(8000000)
//      .select($"uid", expr("0").alias("d901")
//        , expr("0").alias("d902")
//        , expr("0").alias("d903")
//        , expr("0").alias("d904")
//        , expr("0").alias("d905")
//        , expr("0").alias("d906")
//        , expr("0").alias("d907")
//        , expr("0").alias("d908"))
//    df9.write.format("org.apache.spark.sql.cassandra")
//      .options(Map("table" -> "uimge_manual_app2", "keyspace" -> "uimg"))
//      .mode(SaveMode.Append).save()
//    val df10 = spark.read.format("org.apache.spark.sql.cassandra")
//      .options(Map("table" -> "new_estates_group", "keyspace" -> "groups"))
//      .load().sort($"label").limit(8000000)
//      .select($"uid", expr("0").alias("d1001")
//        , expr("0").alias("d1002")
//        , expr("0").alias("d1003")
//        , expr("0").alias("d1004")
//        , expr("0").alias("d1005")
//        , expr("0").alias("d1006")
//        , expr("0").alias("d1007")
//        , expr("0").alias("d1008"))
//    df10.write.format("org.apache.spark.sql.cassandra")
//      .options(Map("table" -> "uimge_manual_app2", "keyspace" -> "uimg"))
//      .mode(SaveMode.Append).save()
//    val df11 = spark.read.format("org.apache.spark.sql.cassandra")
//      .options(Map("table" -> "new_game_group", "keyspace" -> "groups"))
//      .load().sort($"label").limit(4000000)
//      .select($"uid", expr("0").alias("d1101")
//        , expr("0").alias("d1102")
//        , expr("0").alias("d1103")
//        , expr("0").alias("d1104")
//        , expr("0").alias("d1105")
//        , expr("0").alias("d1106")
//        , expr("0").alias("d1107")
//        , expr("0").alias("d1108")
//        , expr("0").alias("d1109"))
//    df11.write.format("org.apache.spark.sql.cassandra")
//      .options(Map("table" -> "uimge_manual_app2", "keyspace" -> "uimg"))
//      .mode(SaveMode.Append).save()
//    val df12 = spark.read.format("org.apache.spark.sql.cassandra")
//      .options(Map("table" -> "new_sport_group", "keyspace" -> "groups"))
//      .load().sort($"label").limit(10000000)
//      .select($"uid", expr("0").alias("d1201")
//        , expr("0").alias("d1202")
//        , expr("0").alias("d1203")
//        , expr("0").alias("d1204")
//        , expr("0").alias("d1205")
//        , expr("0").alias("d1206")
//        , expr("0").alias("d1207"))
//    df12.write.format("org.apache.spark.sql.cassandra")
//      .options(Map("table" -> "uimge_manual_app2", "keyspace" -> "uimg"))
//      .mode(SaveMode.Append).save()
//    val df13 = spark.read.format("org.apache.spark.sql.cassandra")
//      .options(Map("table" -> "new_fashion_group", "keyspace" -> "groups"))
//      .load().sort($"label").limit(4000000)
//      .select($"uid", expr("0").alias("d1301")
//        , expr("0").alias("d1302")
//        , expr("0").alias("d1303")
//        , expr("0").alias("d1304")
//        , expr("0").alias("d1305")
//        , expr("0").alias("d1306")
//        , expr("0").alias("d1307")
//        , expr("0").alias("d1308")
//        , expr("0").alias("d1309")
//        , expr("0").alias("d1310"))
//    df13.write.format("org.apache.spark.sql.cassandra")
//      .options(Map("table" -> "uimge_manual_app2", "keyspace" -> "uimg"))
//      .mode(SaveMode.Append).save()
//    val df14 = spark.read.format("org.apache.spark.sql.cassandra")
//      .options(Map("table" -> "new_retail_group", "keyspace" -> "groups"))
//      .load().sort($"label").limit(3000000)
//      .select($"uid", expr("0").alias("d1401")
//        , expr("0").alias("d1402")
//        , expr("0").alias("d1403")
//        , expr("0").alias("d1405")
//        , expr("0").alias("d1406")
//        , expr("0").alias("d1407")
//        , expr("0").alias("d1408")
//        , expr("0").alias("d1409"))
//    df14.write.format("org.apache.spark.sql.cassandra")
//      .options(Map("table" -> "uimge_manual_app2", "keyspace" -> "uimg"))
//      .mode(SaveMode.Append).save()
//    val df15 = spark.read.format("org.apache.spark.sql.cassandra")
//      .options(Map("table" -> "new_beauty_group", "keyspace" -> "groups"))
//      .load().sort($"label").limit(6000000)
//      .select($"uid", expr("0").alias("d1501")
//        , expr("0").alias("d1502")
//        , expr("0").alias("d1503")
//        , expr("0").alias("d1504")
//        , expr("0").alias("d1505"))
//    df15.write.format("org.apache.spark.sql.cassandra")
//      .options(Map("table" -> "uimge_manual_app2", "keyspace" -> "uimg"))
//      .mode(SaveMode.Append).save()
//    val df16 = spark.read.format("org.apache.spark.sql.cassandra")
//      .options(Map("table" -> "new_digit_group", "keyspace" -> "groups"))
//      .load().sort($"label").limit(6000000)
//      .select($"uid", expr("0").alias("d1601")
//        , expr("0").alias("d1602")
//        , expr("0").alias("d1603")
//        , expr("0").alias("d1604")
//        , expr("0").alias("d1605")
//        , expr("0").alias("d1606")
//        , expr("0").alias("d1607")
//        , expr("0").alias("d1608"))
//    df16.write.format("org.apache.spark.sql.cassandra")
//      .options(Map("table" -> "uimge_manual_app2", "keyspace" -> "uimg"))
//      .mode(SaveMode.Append).save()
    val df17 = spark.read.format("org.apache.spark.sql.cassandra")
      .options(Map("table" -> "new_infant_group", "keyspace" -> "groups"))
      .load().sort($"label").limit(10000000)
      .select($"uid", expr("0").alias("d1701")
        , expr("0").alias("d1702")
        , expr("0").alias("d1703")
        , expr("0").alias("d1704"))
    df17.write.format("org.apache.spark.sql.cassandra")
      .options(Map("table" -> "uimge_manual_app2", "keyspace" -> "uimg"))
      .mode(SaveMode.Append).save()
    val df18 = spark.read.format("org.apache.spark.sql.cassandra")
      .options(Map("table" -> "new_health_group", "keyspace" -> "groups"))
      .load().sort($"label").limit(10000000)
      .select($"uid", expr("0").alias("d1801")
        , expr("0").alias("d1802")
        , expr("0").alias("d1803")
        , expr("0").alias("d1804")
        , expr("0").alias("d1805"))
    df18.write.format("org.apache.spark.sql.cassandra")
      .options(Map("table" -> "uimge_manual_app2", "keyspace" -> "uimg"))
      .mode(SaveMode.Append).save()
  }
}
